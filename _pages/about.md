---
permalink: /
title: "Yung-Sung Chuang"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am now an undergraduate student in Electrical Engineering at National Taiwan University. My research interest broadly covers the deep learning technique for natural language processing and speech processing. In particular, I aim to utilize the ability of machines to help people grasp large information in text/audio form in efficient ways. Towards this goal, I joined Speech Processing Lab supervised by [Hung-Yi Lee](http://speech.ee.ntu.edu.tw/~tlkagk/index.html) and Machine Intelligence Understanding Lab supervised by [Yun-Nung (Vivian) Chen](https://www.csie.ntu.edu.tw/~yvchen/). I received the NTU Presidential Award for top 5% students four times in 2018-2020, [Irving T. Ho Memorial Scholarship](https://irvingthofoundation.github.io/ho-fellows.htm) in 2018 and 2019. Here is my [Curriculum Vitae](/CV/CV.pdf).

I will share some notes or articles during my study in my [blog](https://voidism.github.io). Thanks for reading and taking the time to comment :)

Publications
===

$^\dagger$ indicates equal contribution.


<table>

<tr>
    <td><img src="https://i.imgur.com/Lbw7MIT.png" alt="MDS" width="300"></td>
    <td style="font-size: 17px; line-height: 1.4em;">
<a href="https://arxiv.org/abs/2004.09205">A Study of Cross-Lingual Ability and Language-specific Information in Multilingual BERT</a><br/>Chi-Liang Liu<sup>&#8224;</sup>, Tsung-Yuan Hsu<sup>&#8224;</sup>, <strong>Yung-Sung Chuang<sup>&#8224;</sup></strong>, Hung-Yi Lee. <br/><i>Accepted by RepL4NLP at ACL 2020; arXiv Preprint <a href="https://arxiv.org/abs/2004.09205">https://arxiv.org/abs/2004.09205</a></i></td>
</tr>
<tr>
    <td><img src="https://i.imgur.com/I4fA5uK.png" alt="SpeechBERT" width="300"></td>
    <td style="font-size: 17px; line-height: 1.4em;">
<a href="https://arxiv.org/abs/1910.11559">SpeechBERT: An Audio-and-text Jointly Learned Language Model for End-to-end Spoken Question Answering</a><br/> <strong>Yung-Sung Chuang</strong>, Chi-Liang Liu , Hung-Yi Lee, Lin-shan Lee. <br/><i>Accepted by Interspeech 2020; arXiv Preprint <a href="https://arxiv.org/abs/1910.11559">https://arxiv.org/abs/1910.11559</a></i></td>
</tr>
<tr>
    <td><img src="https://i.imgur.com/sTKvjNv.png" alt="RCT-Gen" width="300"></td>
    <td style="font-size: 17px; line-height: 1.4em;"><a href="https://www.aclweb.org/anthology/D19-6214.pdf">Towards Understanding of Medical Randomized Controlled Trials by Conclusion Generation</a> <br/> Alexander Te-Wei Shieh<sup>&#8224;</sup>, <strong>Yung-Sung Chuang</strong><sup>&#8224;</sup>, Shang-Yu Su, Yun-Nung Chen.<br/> In <i>Proceedings of the 10th International Workshop on Health Text Mining and Information Analysis at EMNLP (LOUHI 2019) </i></td>
</tr>
</table>

  
   

Short Talks
===
## [Non-Autoregressive Conditional Sequence Modeling](https://www.youtube.com/embed/jvyKmU4OM3c)
<iframe width="560" height="315" src="https://www.youtube.com/embed/jvyKmU4OM3c" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## [Unsupervised Syntactic Parsing](https://www.youtube.com/watch?v=YIuBHB9Ejok)
<iframe width="560" height="315" src="https://www.youtube.com/embed/YIuBHB9Ejok" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

  
  

  

  
  
Teaching
===

**Teaching Assistant**, *Deep Learning for Human Language Processing, 2020 Spring* (Prof, Hung-Yi Lee)

  

Current Research Topics
===

- Natural Language Genration: [Text Style Transfer (GAN-based)](https://github.com/voidism/Transformer_CycleGAN_Text_Style_Transfer-pytorch)/[(Delete-Insert-based)](https://github.com/voidism/Insert-Delete-TextStyleTransfer), [Conclusion Generation for Medical RCTs](https://arxiv.org/abs/1910.01462)
- Cross-Modal Language Representations for Text and Speech: [in unsupervised setting](https://docs.google.com/presentation/d/1FWit7TqILpk4ZrcOZ08hV8xuQCdqHub3_xz0vUEWTGY/edit?usp=sharing) / [in supervised settings to solve Spoken Question Answering](https://arxiv.org/abs/1910.11559)
- Non-autoregressive model for Speech-to-Text Translation (in progress)
- Knowledge Distillation & Lifelong learning (in progress)

Projects
===
<table>
<tr>
    <td><img src="https://i.imgur.com/AQTfGQb.png" alt="MIPS" width="300"></td>
    <td style="font-size: 17px; line-height: 1.4em;">
<a href="https://drive.google.com/file/d/199N-2Up1SFS_BtM_nRVw8yCiYuepw1bg/view">Speech Recognition for Impaired Speaker</a><br/><strong>Yung-Sung Chuang</strong>, Tsai-Lun Yang, Che-Ruei Hsu, Liang-Yuan Wu. <br/>Final Project in <i>Introduction to Biomedical Engineering 2020 Spring</i><br/>Reducing WER from 80% to 20% for patient voice via personalized adaptation (in Mandarin).</td>
</tr>
<tr>
    <td><img src="https://i.imgur.com/qR8Wzn1.png" alt="NMLAB" width="300"></td>
    <td style="font-size: 17px; line-height: 1.4em;">
<a href="https://github.com/bchao1/108-2-NMLAB-Final">DPP: Decentralized Publishing Platform</a><br/><strong>Yung-Sung Chuang</strong>, Chung-Hao Chao, Siang-Ruei Wu. <br/>Final Project in <i>Networking and Multinmedia Lab 2020 Spring</i><br/>A Decentralized Publishing Platform created with Blockchain and Etheruem smart contract.</td>
</tr>
<tr>
    <td><img src="https://i.imgur.com/IZNIKYr.png" alt="MIPS" width="300"></td>
    <td style="font-size: 17px; line-height: 1.4em;">
<a href="https://github.com/w4n9r3ntru3/MIPS-Processor">Single-Cycle MIPS Processor Implementation in Verilog</a><br/><strong>Yung-Sung Chuang</strong>, Ren-Chu Wang. <br/>Final Project in <i>Computer Architecture 2019 Fall</i><br/>Ranking 2nd place out of 44 groups by A*T value (Area x Clock Time)</td>
</tr>
<tr>
    <td><img src="https://i.imgur.com/cSgAOjI.png" alt="ICCAD" width="300"></td>
    <td style="font-size: 17px; line-height: 1.4em;">
<a href="https://github.com/raywu0123/ICCAD2019-Problem-E">Rectilinear Polygon Operations for Physical Design</a><br/>Siang-Ruei Wu, <strong>Yung-Sung Chuang</strong>.<br/>Final Project in <i>Algorithms 2019 Spring</i><br/>ICCAD 2019 CAD Contest - Problem E (<a href="http://iccad-contest.org/2019/tw/problems.html">competition website</a>)</td>
</tr>
<tr>
    <td><img src="https://i.imgur.com/oJ7bwsx.png" alt="MDDA" width="300"></td>
    <td style="font-size: 17px; line-height: 1.4em;">
<a href="https://voidism.github.io/pdfs/2019_DLCV_Final_Project_Poster_(final).pdf">Multi-Source Unsupervised Domain Adaptation Challenge</a><br/><strong>Yung-Sung Chuang</strong>, Chen-Yi Lan, Hung-Ting Chen, Chang-Le Liu. <br/>Final Project in Deep Learning for <i>Computer Vision 2019 Spring</i></td>
</tr>
<tr>
    <td><img src="https://i.imgur.com/rJVZ9pc.png" alt="CycleGAN" width="300"></td>
    <td style="font-size: 17px; line-height: 1.4em;">
<a href="https://github.com/voidism/Transformer_CycleGAN_Text_Style_Transfer-pytorch">Transformer-based CycleGAN Text Style Transfer</a><br/><strong>Yung-Sung Chuang</strong>. <br/>Research Project in 2018 Fall</td>
</tr>
<tr>
    <td><img src="https://i.imgur.com/DaR48di.png" alt="PyWordSeg" width="300"></td>
    <td style="font-size: 17px; line-height: 1.4em;">
<a href="https://arxiv.org/abs/1901.05816">Robust Chinese Word Segmentation with Contextualized Word Representations</a><br/><strong>Yung-Sung Chuang</strong>. <br/>ArXiv Link: <a href="https://arxiv.org/abs/1901.05816">https://arxiv.org/abs/1901.05816</a><br/>Pypi Link: <a href="https://pypi.org/project/pywordseg/">https://pypi.org/project/pywordseg/</a><br/>Final project in <i>Digital Signal Processing 2018 Fall</i></td>
</tr>
<tr>
    <td><img src="https://i.imgur.com/ZMwm99W.png" alt="BiParser" width="300"></td>
    <td style="font-size: 17px; line-height: 1.4em;">
<a href="https://github.com/voidism/BiLSTM_Collocation_Parser">BiLSTM Collocation Parser</a><br/><strong>Yung-Sung Chuang</strong>. <br/>Intern work in <i>Acadamia Sinica, 2018 Fall</i></td>
</tr>
<tr>
    <td><img src="https://i.imgur.com/fCFiqu6.png" alt="MTRS" width="300"></td>
    <td style="font-size: 17px; line-height: 1.4em;">
<a href="https://voidism.github.io/project/2018/07/06/Some-Baseline-Methods-for-Multi-Turn-Response-Selection/">Chinese Multi Turn Response Selection</a><br/><strong>Yung-Sung Chuang</strong>, Hsing-Chien Wang, Yi-Ting Hsieh. <br/>Final Project in <i>Machine Learning 2018 Spring</i></td>
</tr>
<tr>
    <td><img src="https://i.imgur.com/URLbmf1.png" alt="MTRS" width="300"></td>
    <td style="font-size: 17px; line-height: 1.4em;">
<a href="https://github.com/voidism/Big-Two">Big Two Game Environment and Agent in C++</a><br/><strong>Yung-Sung Chuang</strong>, Hsin Cheng. <br/>Final Project in <i>Computer Programming 2017 Fall</i></td>
</tr>
</table>

Competitions
===
## NCTS Health Hackathon 2018
*1st Placewith NT$120,000 (out of 18 teams)* \[[News](https://www.ctimes.com.tw/DispNews-tw.asp?O=HK259B39BE2SAA00N9), [Certificate](https://voidism.github.io/img/NCTS_certificate.jpg)\]
- A hackathon on organized byNational Center for Theoretical Sciences and Mount Sinai Health System, New York.
- Proposed an improved system for doctors shifting in hospital â€” PRO (Patient Relay Optimizer) to help doctors grasp all infoabout patients, status, tasks at a glance, reducing the risk of information shifting incompletely.
- Won the 1st place of 2018 H. Spectrum Demo Day (out of 21 teams) \[[News](https://www.bnext.com.tw/article/49900/h.-spectrum-demoday-foxcon)\]

## MakeNTU 2018
*Best Tech Award with NT$50,000 & Microsoft Enterprise Award (out of 50 teams)* \[[Photo](https://goo.gl/wa5LPz)\]
- A hackathon focus on the combination of hardware and software, organized by NTU
- Built an automatic machine for picking good coffee beans with deep learning technique.
- Placed intop 8in the finalist of Microsoft Imagine Cup Taiwan National Final 2018.

## HackNTU 2017
*1st Place of Department of Transportation with NT$50,000 (out of 100+ teams)* \[[Photo](https://goo.gl/tEpX7X)\]
- Built asmart bus bell system for solving the problems of getting on the right bus in the huge and busy city.
- Exhibited on WCIT 2017 (World Congresson Information Technology).